<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Paper Reading 1 (ML)</title>
    <link href="/2023/10/31/Paper-Reading%201-ML/"/>
    <url>/2023/10/31/Paper-Reading%201-ML/</url>
    
    <content type="html"><![CDATA[<h3 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h3><ul><li>论文：Evolutionary game theory and multi-agent reinforcement learning</li><li>概述：这是一篇综述性论文，分别介绍了 <strong>Evolutionary Game Theory</strong>（以下简称<strong>EGT</strong>）和 <strong>Reinforcement Learning</strong>（以下简称<strong>RL</strong>）以及其综合交叉在 <strong>Multi-Agent Reinforcement Learning</strong>（以下简称<strong>MARL</strong>）中的应用。</li></ul><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>该论文的出发点是：在多智能体环境的不确定性前提下，如何令其更好地学习和适应这一问题。解释如下：</p><ul><li>多智能体环境是指有多个智能体（例如机器人、人工智能、人类等）相互交互和影响的环境。例如，一个足球比赛就是一个多智能体环境，因为有两支球队的球员和裁判都在场上活动和决策。</li><li>不确定性是指在多智能体环境中，一个智能体不能完全知道或控制其他智能体的行为、目标、信念和偏好。例如，一个足球队员不能完全预测对方队员会如何传球或射门，也不能完全控制自己队友会如何配合或防守。</li><li>学习和适应是指一个智能体能够根据多智能体环境中的信息和反馈，改进自己的策略和行为，以达到自己的目标。例如，一个足球队员能够根据对方队员的动作和位置，调整自己的跑位和传球，以增加进球的可能性。</li></ul><p><strong>RL</strong> 已经在单智能体环境下取得重大应用成果，但其不能直接扩展到多智能体环境下。这是因为 <strong>RL</strong> 需要环境满足 <strong>Markov</strong> 条件时，其才能保证收敛性。解释如下：</p><ul><li>一个智能体所面对的环境是马尔可夫的，也就是说，环境的下一个状态只取决于当前的状态和智能体的动作，而不受之前的历史影响，而且智能体能够有足够的机会去尝试不同的动作，那么强化学习就能保证智能体最终学到最优的策略。</li></ul><p>然而，在多智能体环境下，一个智能体所获得的强化可能取决于系统中其他代理所采取的行动。因此，<strong>Markov</strong> 条件不再成立。也就不能保证 <strong>RL</strong> 的收敛性。</p><p>这时，我们便需要借助 <strong>EGT</strong> 开发新的算法以得到更好的结果。<strong>Replicator Equation</strong> 是研究各种环境下学习的一个模型。该模型由微分方程系统组成，描述了策略种群（或概率分布）如何随时间演变，在生物和经济模型中起着核心作用。</p><h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><h4 id="RL基础"><a href="#RL基础" class="headerlink" title="RL基础"></a>RL基础</h4><p>强化学习的目标是发现一种策略，即从情况到行动的映射，从而最大限度地提高它所获得的强化。强化是一个标量值，通常用负值表示惩罚，用正值表示奖励。与监督学习不同，RL不需要有标签的数据，而是通过与环境交互，观察和反馈结果来从经验中学习。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
