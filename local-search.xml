<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Book Reading 1 (DL/CNN)</title>
    <link href="/2023/10/31/Book-Reading-1-DL-CNN/"/>
    <url>/2023/10/31/Book-Reading-1-DL-CNN/</url>
    
    <content type="html"><![CDATA[<h3 id="章节概述"><a href="#章节概述" class="headerlink" title="章节概述"></a>章节概述</h3><ul><li>书籍：Dive into Deep Learning (pytorch)</li><li>本章主要介绍卷积神经网络 (convolutional neural network，CNN) 是一类强大的、为处理图像数据而设计的神经网络。我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>不变性<ul><li><strong>平移不变性 (translation invariance)</strong>:不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li>首先，多层感知机的输入是二维图像 $X$，其隐藏表示 $H$ 在数学上是一个矩阵，在代码中表示为二维张量。其中 $X$ 和 $H$ 具有相同的形状。为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。</li><li>使用 $[X]_{ij}$ 和 $[H]_{ij}$分别作为图像和隐藏表示中位置为 $(i, j)$ 中的像素。</li><li>全连接层的隐藏表示为 $[H]_{ij} = \sum_{k}\sum_{l}[W]_{ijkl}[X]_{kl} + [U]_{ij}$</li><li>考虑平移不变性，则隐藏表示为 $[H]_{ij} = \sum_k\sum_l[W]_{kl}[X]_{kl} + [U]$</li><li>这意味着检测对象在输入 $X$ 中的平移，应该仅导致隐藏表示 $H$ 中的平移。也就是说，$W$ 和 $U$ 实际上不依赖于 $(i, j)$ 的值。</li></ul></li><li>局部性<ul><li><strong>局部性 (locality)</strong>: 神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li><li>为了收集用来训练参数 $[H]_{i,j}$ 的相关信息，我们不应偏离到距 $(i, j)$ 很远的地方。</li><li>考虑局部性，则隐藏表示为 $[H]_{ij} = \sum_{-\Delta}^{\Delta}\sum_{-\Delta}^{\Delta}[W]_{ab}[X]_{i+a,j+b} + [U]$</li><li>简而言之，上述隐藏表示是一个<strong>卷积层 (convolutional layer)</strong>，而卷积神经网络是包含卷积层的一类特殊的神经网络。在深度学习研究社区中，V被称为<strong>卷积核 (convolution kernel)</strong>或者<strong>滤波器 (filter)</strong>，亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。</li></ul></li><li>评述 1<ul><li>当图像处理的局部区域很小时，卷积神经网络与多层感知 机的训练差异可能是巨大的:以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神 经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。</li><li>参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。以上所有的权重学习都将 依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到 未知数据中。但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。</li><li>CNN主要依靠平移不变性与局部性对全连接神经网络参数结构进行简化，同时保留其空间结构。但这个当这两个性质不满足时效果将大打折扣。</li></ul></li><li>卷积<ul><li>在数学中, 两个函数 (比如 $f, g$ : $\left.\mathbb{R}^d \rightarrow \mathbb{R}\right)$ 之间的 “卷积” 被定义为$(f * g)(x)=\int_{-\infty}^{+\infty} f(z) g(x-z) dz$</li><li>一维离散形式：$(f*g)(i) = \sum_{a}f(a)g(i-a)$</li><li>二维离散形式：$(f*g)(i,j) = \sum_a\sum_bf(a,b)g(i-a,j-b)$</li><li>对比隐藏表示：$[H]_{ij} = \sum_{-\Delta}^{\Delta}\sum_{-\Delta}^{\Delta}[W]_{ab}[X]_{i+a,j+b} + [U]$ 可以发现这里不是使用 $(i + a, j + b)$，而是使用差值。其运算实际为互相关运算，但与卷积运算匹配。</li></ul></li><li>通道<ul><li>考虑到图像的颜色，其值表示为三种原色，所以图像表示为 $[X]_{lmn}$，对应卷积表示为 $[W]_{abc}$，隐藏表示为 $[H]_{ijk}$。这可以看作一些相互堆叠的二维网络。可以将其想象为一系列具有二维张量的 <strong>通道（Channel）</strong>。这些通道有时也被称为 <strong>特征映射（Feature Maps）</strong>。因为每个通道都向后续层提供一组空间化的学习特征。多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。</li><li>隐藏表示为：$[H]_{ijd} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [W]_{abcd}[X]_{i+a,j+b,c}$，其中隐藏表示 $H$ 中的索引 $d$ 表示输出通道，而随后的输出将继续以三维张量$H$ 作为输入进入下一个卷积层。</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Book</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper Reading 1 (ML)</title>
    <link href="/2023/10/31/Paper-Reading%201-ML/"/>
    <url>/2023/10/31/Paper-Reading%201-ML/</url>
    
    <content type="html"><![CDATA[<h3 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h3><ul><li>论文：Evolutionary game theory and multi-agent reinforcement learning</li><li>概述：这是一篇综述性论文，分别介绍了 <strong>Evolutionary Game Theory</strong>（以下简称<strong>EGT</strong>）和 <strong>Reinforcement Learning</strong>（以下简称<strong>RL</strong>）以及其综合交叉在 <strong>Multi-Agent Reinforcement Learning</strong>（以下简称<strong>MARL</strong>）中的应用。</li></ul><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>该论文的出发点是：在多智能体环境的不确定性前提下，如何令其更好地学习和适应这一问题。解释如下：</p><ul><li>多智能体环境是指有多个智能体（例如机器人、人工智能、人类等）相互交互和影响的环境。例如，一个足球比赛就是一个多智能体环境，因为有两支球队的球员和裁判都在场上活动和决策。</li><li>不确定性是指在多智能体环境中，一个智能体不能完全知道或控制其他智能体的行为、目标、信念和偏好。例如，一个足球队员不能完全预测对方队员会如何传球或射门，也不能完全控制自己队友会如何配合或防守。</li><li>学习和适应是指一个智能体能够根据多智能体环境中的信息和反馈，改进自己的策略和行为，以达到自己的目标。例如，一个足球队员能够根据对方队员的动作和位置，调整自己的跑位和传球，以增加进球的可能性。</li></ul><p><strong>RL</strong> 已经在单智能体环境下取得重大应用成果，但其不能直接扩展到多智能体环境下。这是因为 <strong>RL</strong> 需要环境满足 <strong>Markov</strong> 条件时，其才能保证收敛性。解释如下：</p><ul><li>一个智能体所面对的环境是马尔可夫的，也就是说，环境的下一个状态只取决于当前的状态和智能体的动作，而不受之前的历史影响，而且智能体能够有足够的机会去尝试不同的动作，那么强化学习就能保证智能体最终学到最优的策略。</li></ul><p>然而，在多智能体环境下，一个智能体所获得的强化可能取决于系统中其他代理所采取的行动。因此，<strong>Markov</strong> 条件不再成立。也就不能保证 <strong>RL</strong> 的收敛性。</p><p>这时，我们便需要借助 <strong>EGT</strong> 开发新的算法以得到更好的结果。<strong>Replicator Equation</strong> 是研究各种环境下学习的一个模型。该模型由微分方程系统组成，描述了策略种群（或概率分布）如何随时间演变，在生物和经济模型中起着核心作用。</p><h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><h4 id="RL基础"><a href="#RL基础" class="headerlink" title="RL基础"></a>RL基础</h4><h5 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h5><p>强化学习的目标是发现一种策略，即从情况到行动的映射，从而最大限度地提高它所获得的强化。强化是一个标量值，通常用负值表示惩罚，用正值表示奖励。与监督学习不同，RL不需要有标签的数据，而是通过与环境交互，观察和反馈结果来从经验中学习。在此基础部分，我们先考虑单智能体的情况。</p><h5 id="RL-amp-DP"><a href="#RL-amp-DP" class="headerlink" title="RL &amp; DP"></a>RL &amp; DP</h5><p><strong>RL</strong> 从数学的视角来看，与 <strong>Dynamic Programming</strong>（以下简称 <strong>DP</strong>）的关系十分密切。</p><ul><li><strong>DP</strong> 是一种解决 <strong>Markov Decision Problem</strong>（以下简称 <strong>MDP</strong>）的方法。<strong>MDP</strong> 是一个多阶段决策问题，即找到预期长期回报最优的策略。</li><li><strong>Markovian Property</strong> 保证了只观察当前状态就能表现得最优，也就是说，不需要跟踪历史。</li><li>在  <strong>Markovian Property</strong> 下，环境是随机的，其响应由一个转移矩阵来描述。这个矩阵给出了在当前状态为 $s_t$，所采取的行动为 $a_t$ 的情况下，到达下一个状态 $s_{t+1}$ 并获得奖励 $r_{t+1}$的概率。</li><li>转移概率表示为：$P_{s s^{\prime}}^a=P\left\{s_{t+1}=s^{\prime} \mid s_t=s, a_t=a\right\}$。</li><li>下一个奖励的期望值是：$R_{s s^{\prime}}^a=E\left\{r_{t+1} \mid s_t=s, a_t=a, s_{t+1}=s^{\prime}\right\}$</li><li><p><strong>DP</strong> 通常分为两种方法: <strong>PI</strong> 方法和 <strong>VI</strong> 方法，对应的，<strong>RL</strong>同理。</p></li><li><p><strong>PI</strong> 方法考虑当前策略，并尝试根据当前策略对应的状态值对策略进行局部改进。即：$\begin{aligned} V^\pi(s) &amp; =E_\pi\left\{r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\cdots \mid s_t=s\right\} \\ &amp; =E_\pi\left\{r_{t+1}+\gamma V^\pi\left(s_{t+1}\right) \mid s_t=s\right\}\end{aligned}$ </p></li><li>该值函数为迭代定义，再将转移概率与奖励期望代入，可解得最优解为：$V^<em>(s)=\max _a \sum_{s^{\prime}} P_{s s^{\prime}}^a\left[R_{s s^{\prime}}^a+\gamma V^</em>\left(s^{\prime}\right)\right]$</li><li>该最优解以下式迭代得到：$\begin{aligned} &amp; V_{k+1}(s)=E_\pi\left\{r_{t+1}+\gamma V_{k+1}\left(s_{t+1}\right) \mid s_t=s\right\} \\ &amp; =\sum_a \pi(s, a) \sum_{s^{\prime}} P_{s s^{\prime}}^a\left[R_{S s^{\prime}}^a+\gamma V_k\left(s^{\prime}\right)\right] .\end{aligned}$</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
