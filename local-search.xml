<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Book Reading 1 (DL/CNN)</title>
    <link href="/2023/10/31/Book-Reading-1-DL-CNN/"/>
    <url>/2023/10/31/Book-Reading-1-DL-CNN/</url>
    
    <content type="html"><![CDATA[<h3 id="章节概述"><a href="#章节概述" class="headerlink" title="章节概述"></a>章节概述</h3><ul><li>书籍：Dive into Deep Learning (pytorch)</li><li>本章主要介绍卷积神经网络 (convolutional neural network，CNN) 是一类强大的、为处理图像数据而设计的神经网络。我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>不变性<ul><li><strong>平移不变性 (translation invariance)</strong>:不管检测对象出现在图像中的哪个位置，神经网络的前面几 应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li>首先，多层感知机的输入是二维图像 $X$，其隐藏表示 $H$ 在数学上是一个矩阵，在代码中表示为二维张量。其中 $X$ 和 $H$ 具有相同的形状。为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。</li><li>使用 $[X]_{ij}$ 和 $[H]_{ij}$分别作为图像和隐藏表示中位置为 $(i, j)$ 中的像素。</li><li>全连接层的隐藏表示为 $[H]_{ij} = \sum_{k}\sum_{l}[W]_{ijkl}[X]_{kl} + [U]_{ij}$</li><li>考虑平移不变性，则隐藏表示为 $[H]_{ij} = \sum_k\sum_l[W]_{kl}[X]_{kl} + [U]$</li><li>这意味着检测对象在输入X中的平移，应该仅导致隐藏表示H中的平移。也就是说，$W$ 和 $U$ 实际上不依赖于 $(i, j)$ 的值。</li></ul></li><li><p>局部性</p><ul><li><strong>局部性 (locality)</strong>: 神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li><li>为了收集用来训练参数 $[H]_{i,j}$ 的相关信息，我们不应偏离到距 $(i, j)$ 很远的地方。</li><li>考虑局部性，则隐藏表示为 $[H]_{ij} = \sum_{-\Delta}^{\Delta}\sum_{-\Delta}^{\Delta}[W]_{ab}[X]_{i+a,j+b} + [U]$</li><li>简而言之，上述隐藏表示是一个<strong>卷积层 (convolutional layer)</strong>，而卷积神经网络是包含卷积层的一类特殊的神经网络。在深度学习研究社区中，V被称为<strong>卷积核 (convolution kernel)</strong>或者<strong>滤波器 (filter)</strong>，亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数。</li></ul></li><li><p>评述 1</p><ul><li>当图像处理的局部区域很小时，卷积神经网络与多层感知 机的训练差异可能是巨大的:以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神 经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。</li><li>参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。以上所有的权重学习都将 依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到 未知数据中。但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。</li><li>CNN主要依靠平移不变性与局部性对全连接神经网络参数结构进行简化，同时保留其空间结构。但这个当这两个性质不满足时效果将大打折扣。</li></ul></li><li><p>卷积</p><ul><li>在数学中, 两个函数 (比如 $f, g$ : $\left.\mathbb{R}^d \rightarrow \mathbb{R}\right)$ 之间的 “卷积” 被定义为$(f * g)(x)=\int_{-\infty}^{+\infty} f(z) g(x-z) dz$</li><li>一维离散形式：$(f*g)(i) = \sum_{a}f(a)g(i-a)$</li><li>二维离散形式：$(f*g)(i,j) = \sum_a\sum_bf(a,b)g(i-a,j-b)$</li><li>对比隐藏表示：$[H]_{ij} = \sum_{-\Delta}^{\Delta}\sum_{-\Delta}^{\Delta}[W]_{ab}[X]_{i+a,j+b} + [U]$ 可以发现这里不是使用 $(i + a, j + b)$，而是使用差值。</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Book</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Paper Reading 1 (ML)</title>
    <link href="/2023/10/31/Paper-Reading%201-ML/"/>
    <url>/2023/10/31/Paper-Reading%201-ML/</url>
    
    <content type="html"><![CDATA[<h3 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h3><ul><li>论文：Evolutionary game theory and multi-agent reinforcement learning</li><li>概述：这是一篇综述性论文，分别介绍了 <strong>Evolutionary Game Theory</strong>（以下简称<strong>EGT</strong>）和 <strong>Reinforcement Learning</strong>（以下简称<strong>RL</strong>）以及其综合交叉在 <strong>Multi-Agent Reinforcement Learning</strong>（以下简称<strong>MARL</strong>）中的应用。</li></ul><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>该论文的出发点是：在多智能体环境的不确定性前提下，如何令其更好地学习和适应这一问题。解释如下：</p><ul><li>多智能体环境是指有多个智能体（例如机器人、人工智能、人类等）相互交互和影响的环境。例如，一个足球比赛就是一个多智能体环境，因为有两支球队的球员和裁判都在场上活动和决策。</li><li>不确定性是指在多智能体环境中，一个智能体不能完全知道或控制其他智能体的行为、目标、信念和偏好。例如，一个足球队员不能完全预测对方队员会如何传球或射门，也不能完全控制自己队友会如何配合或防守。</li><li>学习和适应是指一个智能体能够根据多智能体环境中的信息和反馈，改进自己的策略和行为，以达到自己的目标。例如，一个足球队员能够根据对方队员的动作和位置，调整自己的跑位和传球，以增加进球的可能性。</li></ul><p><strong>RL</strong> 已经在单智能体环境下取得重大应用成果，但其不能直接扩展到多智能体环境下。这是因为 <strong>RL</strong> 需要环境满足 <strong>Markov</strong> 条件时，其才能保证收敛性。解释如下：</p><ul><li>一个智能体所面对的环境是马尔可夫的，也就是说，环境的下一个状态只取决于当前的状态和智能体的动作，而不受之前的历史影响，而且智能体能够有足够的机会去尝试不同的动作，那么强化学习就能保证智能体最终学到最优的策略。</li></ul><p>然而，在多智能体环境下，一个智能体所获得的强化可能取决于系统中其他代理所采取的行动。因此，<strong>Markov</strong> 条件不再成立。也就不能保证 <strong>RL</strong> 的收敛性。</p><p>这时，我们便需要借助 <strong>EGT</strong> 开发新的算法以得到更好的结果。<strong>Replicator Equation</strong> 是研究各种环境下学习的一个模型。该模型由微分方程系统组成，描述了策略种群（或概率分布）如何随时间演变，在生物和经济模型中起着核心作用。</p><h3 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h3><h4 id="RL基础"><a href="#RL基础" class="headerlink" title="RL基础"></a>RL基础</h4><p>强化学习的目标是发现一种策略，即从情况到行动的映射，从而最大限度地提高它所获得的强化。强化是一个标量值，通常用负值表示惩罚，用正值表示奖励。与监督学习不同，RL不需要有标签的数据，而是通过与环境交互，观察和反馈结果来从经验中学习。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
